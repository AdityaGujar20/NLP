{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "774eb7a1-ed68-4b78-81d9-a28c6fbcd888",
   "metadata": {},
   "source": [
    "**Name**: Aditya Gujar  \n",
    "**Reg no.**: 2448505    \n",
    "**Subject**: Natural Language Processing  \n",
    "**Date**: 04-02-2025    \n",
    "**Practical-1 (Tokenization)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a307a09d",
   "metadata": {},
   "source": [
    "# Tokenization  \n",
    "\n",
    "## What is Tokenization?  \n",
    "Tokenization is the process of breaking down text into smaller units called **tokens**. These tokens can be words, sentences, or subwords, depending on the type of tokenization used. It is a fundamental step in **Natural Language Processing (NLP)**.\n",
    "\n",
    "## Types of Tokenization:  \n",
    "1. **Word Tokenization** â€“ Splits text into individual words.  \n",
    "2. **Sentence Tokenization** â€“ Divides text into sentences.  \n",
    "3. **Punctuation-based Tokenization** â€“ Splits text using punctuation marks.  \n",
    "4. **Subword Tokenization** â€“ Breaks words into meaningful subunits (useful for deep learning models).  \n",
    "\n",
    "## Why is Tokenization Important?  \n",
    "- Prepares text for **NLP tasks** like sentiment analysis, chatbots, and search engines.  \n",
    "- Helps in **text normalization** by handling punctuation, contractions, and stop words.  \n",
    "- Essential for **machine learning models** to process text effectively.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "678e23b4-a057-4e9d-9dc9-02bd3bb4b99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph = \"Artificial Intelligence (AI) is revolutionizing industries, from healthcare ğŸ¥ to finance ğŸ“‰, and even creative fields like music ğŸµ and art ğŸ¨. However, AI doesnâ€™t always get it rightâ€”sometimes it *doesnâ€™t* understand context or nuances, leading to errors. For example, sentiment analysis tools might misinterpret sarcasm or negation (e.g., 'I donâ€™t like this product ğŸ˜’ğŸ˜¤'). Despite these challenges, AI continues to evolve, offering solutions like personalized recommendations, fraud detection, and even autonomous vehicles ğŸš—ğŸ¤–. The future of AI is bright, but itâ€™s crucial to address ethical concerns and biases to ensure fairness and inclusivity.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d20bbc",
   "metadata": {},
   "source": [
    "## a. Word Tokenization (NLTK)\n",
    "\n",
    "- Splits text into individual words.\n",
    "- Retains punctuation and handles contractions.\n",
    "- Example: `[\"Artificial\", \"Intelligence\", \"(\", \"AI\", \")\"]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7a30254-af7d-40a7-94a3-a32828955fbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Tokenization:\n",
      " ['Artificial', 'Intelligence', '(', 'AI', ')', 'is', 'revolutionizing', 'industries', ',', 'from', 'healthcare', 'ğŸ¥', 'to', 'finance', 'ğŸ“‰', ',', 'and', 'even', 'creative', 'fields', 'like', 'music', 'ğŸµ', 'and', 'art', 'ğŸ¨', '.', 'However', ',', 'AI', 'doesn', 'â€™', 't', 'always', 'get', 'it', 'rightâ€”sometimes', 'it', '*', 'doesn', 'â€™', 't', '*', 'understand', 'context', 'or', 'nuances', ',', 'leading', 'to', 'errors', '.', 'For', 'example', ',', 'sentiment', 'analysis', 'tools', 'might', 'misinterpret', 'sarcasm', 'or', 'negation', '(', 'e.g.', ',', \"'\", 'I', 'don', 'â€™', 't', 'like', 'this', 'product', 'ğŸ˜’ğŸ˜¤', \"'\", ')', '.', 'Despite', 'these', 'challenges', ',', 'AI', 'continues', 'to', 'evolve', ',', 'offering', 'solutions', 'like', 'personalized', 'recommendations', ',', 'fraud', 'detection', ',', 'and', 'even', 'autonomous', 'vehicles', 'ğŸš—ğŸ¤–', '.', 'The', 'future', 'of', 'AI', 'is', 'bright', ',', 'but', 'it', 'â€™', 's', 'crucial', 'to', 'address', 'ethical', 'concerns', 'and', 'biases', 'to', 'ensure', 'fairness', 'and', 'inclusivity', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "words = word_tokenize(paragraph)\n",
    "print(\"Word Tokenization:\\n\", words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e4aca4",
   "metadata": {},
   "source": [
    "## b. Sentence Tokenization (NLTK)\n",
    "- Splits text into sentences based on punctuation.\n",
    "- Useful for sentence-level analysis.\n",
    "- Example: `[\"Artificial Intelligence (AI) is revolutionizing industries.\", \"However, AI doesnâ€™t always get it right.\"]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb4b46b8-a9fe-4e1a-8815-6bc083ae4d98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence Tokenization:\n",
      " ['Artificial Intelligence (AI) is revolutionizing industries, from healthcare ğŸ¥ to finance ğŸ“‰, and even creative fields like music ğŸµ and art ğŸ¨.', 'However, AI doesnâ€™t always get it rightâ€”sometimes it *doesnâ€™t* understand context or nuances, leading to errors.', \"For example, sentiment analysis tools might misinterpret sarcasm or negation (e.g., 'I donâ€™t like this product ğŸ˜’ğŸ˜¤').\", 'Despite these challenges, AI continues to evolve, offering solutions like personalized recommendations, fraud detection, and even autonomous vehicles ğŸš—ğŸ¤–.', 'The future of AI is bright, but itâ€™s crucial to address ethical concerns and biases to ensure fairness and inclusivity.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "sentences = sent_tokenize(paragraph)\n",
    "print(\"Sentence Tokenization:\\n\", sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898eb3ab",
   "metadata": {},
   "source": [
    "## c. Punctuation-based Tokenizer (WordPunctTokenizer)\n",
    "- Splits words at punctuation marks.\n",
    "- Example: `[\"Artificial\", \"Intelligence\", \"(\", \"AI\", \")\", \"is\"]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "366fe5e7-0e37-455e-90a3-6a89ea2fbced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Punctuation-based Tokenization:\n",
      " ['Artificial', 'Intelligence', '(', 'AI', ')', 'is', 'revolutionizing', 'industries', ',', 'from', 'healthcare', 'ğŸ¥', 'to', 'finance', 'ğŸ“‰,', 'and', 'even', 'creative', 'fields', 'like', 'music', 'ğŸµ', 'and', 'art', 'ğŸ¨.', 'However', ',', 'AI', 'doesn', 'â€™', 't', 'always', 'get', 'it', 'right', 'â€”', 'sometimes', 'it', '*', 'doesn', 'â€™', 't', '*', 'understand', 'context', 'or', 'nuances', ',', 'leading', 'to', 'errors', '.', 'For', 'example', ',', 'sentiment', 'analysis', 'tools', 'might', 'misinterpret', 'sarcasm', 'or', 'negation', '(', 'e', '.', 'g', '.,', \"'\", 'I', 'don', 'â€™', 't', 'like', 'this', 'product', \"ğŸ˜’ğŸ˜¤').\", 'Despite', 'these', 'challenges', ',', 'AI', 'continues', 'to', 'evolve', ',', 'offering', 'solutions', 'like', 'personalized', 'recommendations', ',', 'fraud', 'detection', ',', 'and', 'even', 'autonomous', 'vehicles', 'ğŸš—ğŸ¤–.', 'The', 'future', 'of', 'AI', 'is', 'bright', ',', 'but', 'it', 'â€™', 's', 'crucial', 'to', 'address', 'ethical', 'concerns', 'and', 'biases', 'to', 'ensure', 'fairness', 'and', 'inclusivity', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "tokenizer = WordPunctTokenizer()\n",
    "punct_tokens = tokenizer.tokenize(paragraph)\n",
    "print(\"Punctuation-based Tokenization:\\n\", punct_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc1147a",
   "metadata": {},
   "source": [
    "## d. Treebank Word Tokenizer\n",
    "- Based on the Penn Treebank dataset.\n",
    "- Handles contractions like `doesn't â†’ does + n't`.\n",
    "- Example: `[\"does\", \"n't\", \"like\"]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6483f66-55f4-41c9-ab77-55549a01cf76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treebank Tokenization:\n",
      " ['Artificial', 'Intelligence', '(', 'AI', ')', 'is', 'revolutionizing', 'industries', ',', 'from', 'healthcare', 'ğŸ¥', 'to', 'finance', 'ğŸ“‰', ',', 'and', 'even', 'creative', 'fields', 'like', 'music', 'ğŸµ', 'and', 'art', 'ğŸ¨.', 'However', ',', 'AI', 'doesnâ€™t', 'always', 'get', 'it', 'rightâ€”sometimes', 'it', '*doesnâ€™t*', 'understand', 'context', 'or', 'nuances', ',', 'leading', 'to', 'errors.', 'For', 'example', ',', 'sentiment', 'analysis', 'tools', 'might', 'misinterpret', 'sarcasm', 'or', 'negation', '(', 'e.g.', ',', \"'I\", 'donâ€™t', 'like', 'this', 'product', 'ğŸ˜’ğŸ˜¤', \"'\", ')', '.', 'Despite', 'these', 'challenges', ',', 'AI', 'continues', 'to', 'evolve', ',', 'offering', 'solutions', 'like', 'personalized', 'recommendations', ',', 'fraud', 'detection', ',', 'and', 'even', 'autonomous', 'vehicles', 'ğŸš—ğŸ¤–.', 'The', 'future', 'of', 'AI', 'is', 'bright', ',', 'but', 'itâ€™s', 'crucial', 'to', 'address', 'ethical', 'concerns', 'and', 'biases', 'to', 'ensure', 'fairness', 'and', 'inclusivity', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "treebank_tokens = tokenizer.tokenize(paragraph)\n",
    "print(\"Treebank Tokenization:\\n\", treebank_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c15de0",
   "metadata": {},
   "source": [
    "## e. Tweet Tokenizer\n",
    "- Designed for social media text (hashtags, emojis, etc.).\n",
    "- Example: `[\"AI\", \"doesn\", \"â€™t\", \"like\", \"this\", \"product\", \"ğŸ˜’\"]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9432e2ce-428c-47db-bc44-b1a45c8c8678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet Tokenization:\n",
      " ['Artificial', 'Intelligence', '(', 'AI', ')', 'is', 'revolutionizing', 'industries', ',', 'from', 'healthcare', 'ğŸ¥', 'to', 'finance', 'ğŸ“‰', ',', 'and', 'even', 'creative', 'fields', 'like', 'music', 'ğŸµ', 'and', 'art', 'ğŸ¨', '.', 'However', ',', 'AI', 'doesn', 'â€™', 't', 'always', 'get', 'it', 'right', 'â€”', 'sometimes', 'it', '*', 'doesn', 'â€™', 't', '*', 'understand', 'context', 'or', 'nuances', ',', 'leading', 'to', 'errors', '.', 'For', 'example', ',', 'sentiment', 'analysis', 'tools', 'might', 'misinterpret', 'sarcasm', 'or', 'negation', '(', 'e', '.', 'g', '.', ',', \"'\", 'I', 'don', 'â€™', 't', 'like', 'this', 'product', 'ğŸ˜’', 'ğŸ˜¤', \"'\", ')', '.', 'Despite', 'these', 'challenges', ',', 'AI', 'continues', 'to', 'evolve', ',', 'offering', 'solutions', 'like', 'personalized', 'recommendations', ',', 'fraud', 'detection', ',', 'and', 'even', 'autonomous', 'vehicles', 'ğŸš—', 'ğŸ¤–', '.', 'The', 'future', 'of', 'AI', 'is', 'bright', ',', 'but', 'it', 'â€™', 's', 'crucial', 'to', 'address', 'ethical', 'concerns', 'and', 'biases', 'to', 'ensure', 'fairness', 'and', 'inclusivity', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "tokenizer = TweetTokenizer()\n",
    "tweet_tokens = tokenizer.tokenize(paragraph)\n",
    "print(\"Tweet Tokenization:\\n\", tweet_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9240bf",
   "metadata": {},
   "source": [
    "## f. Multi-Word Expression Tokenizer (MWETokenizer)\n",
    "- Keeps predefined multi-word expressions together.\n",
    "- Example: `[\"Artificial_Intelligence\", \"(\", \"AI\", \")\"]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "999a3422-2a8c-464f-a064-d810769b59de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MWE Tokenization:\n",
      " ['Artificial_Intelligence', '(', 'AI', ')', 'is', 'revolutionizing', 'industries', ',', 'from', 'healthcare', 'ğŸ¥', 'to', 'finance', 'ğŸ“‰', ',', 'and', 'even', 'creative', 'fields', 'like', 'music', 'ğŸµ', 'and', 'art', 'ğŸ¨', '.', 'However', ',', 'AI', 'doesn', 'â€™', 't', 'always', 'get', 'it', 'rightâ€”sometimes', 'it', '*', 'doesn', 'â€™', 't', '*', 'understand', 'context', 'or', 'nuances', ',', 'leading', 'to', 'errors', '.', 'For', 'example', ',', 'sentiment', 'analysis', 'tools', 'might', 'misinterpret', 'sarcasm', 'or', 'negation', '(', 'e.g.', ',', \"'\", 'I', 'don', 'â€™', 't', 'like', 'this', 'product', 'ğŸ˜’ğŸ˜¤', \"'\", ')', '.', 'Despite', 'these', 'challenges', ',', 'AI', 'continues', 'to', 'evolve', ',', 'offering', 'solutions', 'like', 'personalized', 'recommendations', ',', 'fraud', 'detection', ',', 'and', 'even', 'autonomous_vehicles', 'ğŸš—ğŸ¤–', '.', 'The', 'future', 'of', 'AI', 'is', 'bright', ',', 'but', 'it', 'â€™', 's', 'crucial', 'to', 'address', 'ethical', 'concerns', 'and', 'biases', 'to', 'ensure', 'fairness', 'and', 'inclusivity', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import MWETokenizer\n",
    "\n",
    "tokenizer = MWETokenizer([('Artificial', 'Intelligence'), ('autonomous', 'vehicles')])\n",
    "mwe_tokens = tokenizer.tokenize(word_tokenize(paragraph))\n",
    "print(\"MWE Tokenization:\\n\", mwe_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af572ea",
   "metadata": {},
   "source": [
    "## g. TextBlob Tokenization\n",
    "- Similar to `nltk.word_tokenize` but optimized for NLP.\n",
    "- Example: `[\"Artificial\", \"Intelligence\", \"AI\", \"is\"]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26009a3b-670d-473b-9c24-c78d1f557e54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TextBlob Tokenization:\n",
      " ['Artificial', 'Intelligence', 'AI', 'is', 'revolutionizing', 'industries', 'from', 'healthcare', 'ğŸ¥', 'to', 'finance', 'ğŸ“‰', 'and', 'even', 'creative', 'fields', 'like', 'music', 'ğŸµ', 'and', 'art', 'ğŸ¨', 'However', 'AI', 'doesn', 'â€™', 't', 'always', 'get', 'it', 'rightâ€”sometimes', 'it', 'doesn', 'â€™', 't', 'understand', 'context', 'or', 'nuances', 'leading', 'to', 'errors', 'For', 'example', 'sentiment', 'analysis', 'tools', 'might', 'misinterpret', 'sarcasm', 'or', 'negation', 'e.g', 'I', 'don', 'â€™', 't', 'like', 'this', 'product', 'ğŸ˜’ğŸ˜¤', 'Despite', 'these', 'challenges', 'AI', 'continues', 'to', 'evolve', 'offering', 'solutions', 'like', 'personalized', 'recommendations', 'fraud', 'detection', 'and', 'even', 'autonomous', 'vehicles', 'ğŸš—ğŸ¤–', 'The', 'future', 'of', 'AI', 'is', 'bright', 'but', 'it', 'â€™', 's', 'crucial', 'to', 'address', 'ethical', 'concerns', 'and', 'biases', 'to', 'ensure', 'fairness', 'and', 'inclusivity']\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "blob = TextBlob(paragraph)\n",
    "textblob_tokens = blob.words\n",
    "print(\"TextBlob Tokenization:\\n\", textblob_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43fb5d29",
   "metadata": {},
   "source": [
    "## h. spaCy Tokenizer\n",
    "- Advanced NLP tokenizer with entity recognition.\n",
    "- Handles punctuation, contractions, and spaces.\n",
    "- Example: `[\"Artificial\", \"Intelligence\", \"(\", \"AI\", \")\"]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05b892f0-15c8-4031-beff-765057561e7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy Tokenization:\n",
      " ['Artificial', 'Intelligence', '(', 'AI', ')', 'is', 'revolutionizing', 'industries', ',', 'from', 'healthcare', 'ğŸ¥', 'to', 'finance', 'ğŸ“‰', ',', 'and', 'even', 'creative', 'fields', 'like', 'music', 'ğŸµ', 'and', 'art', 'ğŸ¨', '.', 'However', ',', 'AI', 'does', 'nâ€™t', 'always', 'get', 'it', 'right', 'â€”', 'sometimes', 'it', '*', 'does', 'nâ€™t', '*', 'understand', 'context', 'or', 'nuances', ',', 'leading', 'to', 'errors', '.', 'For', 'example', ',', 'sentiment', 'analysis', 'tools', 'might', 'misinterpret', 'sarcasm', 'or', 'negation', '(', 'e.g.', ',', \"'\", 'I', 'do', 'nâ€™t', 'like', 'this', 'product', 'ğŸ˜’', 'ğŸ˜¤', \"'\", ')', '.', 'Despite', 'these', 'challenges', ',', 'AI', 'continues', 'to', 'evolve', ',', 'offering', 'solutions', 'like', 'personalized', 'recommendations', ',', 'fraud', 'detection', ',', 'and', 'even', 'autonomous', 'vehicles', 'ğŸš—', 'ğŸ¤–', '.', 'The', 'future', 'of', 'AI', 'is', 'bright', ',', 'but', 'it', 'â€™s', 'crucial', 'to', 'address', 'ethical', 'concerns', 'and', 'biases', 'to', 'ensure', 'fairness', 'and', 'inclusivity', '.']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(paragraph)\n",
    "spacy_tokens = [token.text for token in doc]\n",
    "print(\"spaCy Tokenization:\\n\", spacy_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4896890b",
   "metadata": {},
   "source": [
    "## i. Gensim Tokenizer\n",
    "- Efficient word tokenizer that removes punctuation.\n",
    "- Example: `[\"Artificial\", \"Intelligence\", \"AI\"]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72f0d2f3-216c-479f-8f80-8a37fce04c3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gensim Tokenization:\n",
      " ['Artificial', 'Intelligence', 'AI', 'is', 'revolutionizing', 'industries', 'from', 'healthcare', 'to', 'finance', 'and', 'even', 'creative', 'fields', 'like', 'music', 'and', 'art', 'However', 'AI', 'doesn', 't', 'always', 'get', 'it', 'right', 'sometimes', 'it', 'doesn', 't', 'understand', 'context', 'or', 'nuances', 'leading', 'to', 'errors', 'For', 'example', 'sentiment', 'analysis', 'tools', 'might', 'misinterpret', 'sarcasm', 'or', 'negation', 'e', 'g', 'I', 'don', 't', 'like', 'this', 'product', 'Despite', 'these', 'challenges', 'AI', 'continues', 'to', 'evolve', 'offering', 'solutions', 'like', 'personalized', 'recommendations', 'fraud', 'detection', 'and', 'even', 'autonomous', 'vehicles', 'The', 'future', 'of', 'AI', 'is', 'bright', 'but', 'it', 's', 'crucial', 'to', 'address', 'ethical', 'concerns', 'and', 'biases', 'to', 'ensure', 'fairness', 'and', 'inclusivity']\n"
     ]
    }
   ],
   "source": [
    "from gensim.utils import tokenize\n",
    "\n",
    "gensim_tokens = list(tokenize(paragraph))\n",
    "print(\"Gensim Tokenization:\\n\", gensim_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f37d97",
   "metadata": {},
   "source": [
    "## j. Keras Tokenizer\n",
    "- Converts text to lowercase and removes punctuation.\n",
    "- Example: `[\"artificial\", \"intelligence\", \"ai\"]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "80d0cd1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras Tokenization:\n",
      " ['artificial', 'intelligence', 'ai', 'is', 'revolutionizing', 'industries', 'from', 'healthcare', 'ğŸ¥', 'to', 'finance', 'ğŸ“‰', 'and', 'even', 'creative', 'fields', 'like', 'music', 'ğŸµ', 'and', 'art', 'ğŸ¨', 'however', 'ai', 'doesnâ€™t', 'always', 'get', 'it', 'rightâ€”sometimes', 'it', 'doesnâ€™t', 'understand', 'context', 'or', 'nuances', 'leading', 'to', 'errors', 'for', 'example', 'sentiment', 'analysis', 'tools', 'might', 'misinterpret', 'sarcasm', 'or', 'negation', 'e', 'g', \"'i\", 'donâ€™t', 'like', 'this', 'product', \"ğŸ˜’ğŸ˜¤'\", 'despite', 'these', 'challenges', 'ai', 'continues', 'to', 'evolve', 'offering', 'solutions', 'like', 'personalized', 'recommendations', 'fraud', 'detection', 'and', 'even', 'autonomous', 'vehicles', 'ğŸš—ğŸ¤–', 'the', 'future', 'of', 'ai', 'is', 'bright', 'but', 'itâ€™s', 'crucial', 'to', 'address', 'ethical', 'concerns', 'and', 'biases', 'to', 'ensure', 'fairness', 'and', 'inclusivity']\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "keras_tokens = text_to_word_sequence(paragraph)\n",
    "print(\"Keras Tokenization:\\n\", keras_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c74515",
   "metadata": {},
   "source": [
    "## Tokenizer Comparison Table\n",
    "\n",
    "| Index | Tokenizer                      | Handles Punctuation | Handles Contractions | Good for Social Media | NLP Optimized |\n",
    "|-------|--------------------------------|---------------------|----------------------|-----------------------|--------------|\n",
    "| a   | **Word Tokenization (NLTK)**   | âœ… Yes             | âœ… Yes               | âŒ No                 | âœ… Yes       |\n",
    "| b   | **Sentence Tokenization (NLTK)** | âœ… Yes           | âœ… Yes               | âŒ No                 | âœ… Yes       |\n",
    "| c   | **Punctuation-based (WordPunct)** | âœ… Yes         | âŒ No                | âŒ No                 | âœ… Yes       |\n",
    "| d   | **Treebank Tokenizer**         | âœ… Yes             | âœ… Yes               | âŒ No                 | âœ… Yes       |\n",
    "| e   | **Tweet Tokenizer**            | âœ… Yes             | âœ… Yes               | âœ… Yes                | âœ… Yes       |\n",
    "| f   | **Multi-Word Tokenizer**       | âœ… Yes             | âŒ No                | âŒ No                 | âœ… Yes       |\n",
    "| g   | **TextBlob Tokenizer**         | âœ… Yes             | âœ… Yes               | âŒ No                 | âœ… Yes       |\n",
    "| h   | **spaCy Tokenizer**            | âœ… Yes             | âœ… Yes               | âŒ No                 | âœ… Yes       |\n",
    "| i   | **Gensim Tokenizer**           | âŒ No              | âŒ No                | âŒ No                 | âœ… Yes       |\n",
    "| j   | **Keras Tokenizer**            | âŒ No              | âŒ No                | âŒ No                 | âœ… Yes       |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
